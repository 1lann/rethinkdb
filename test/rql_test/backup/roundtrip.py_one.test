#!/usr/bin/python

import os, pprint, random, string, sys, tempfile

sys.path.insert(0, os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir, "common"))
import driver, rdb_unittest, utils

try:
    get_char = unichr
except NameError:
    get_char = chr

def counter(i=0):
    while True:
        i += 1
        yield i

__unicode_alphabet = None
def randomString(length=None, max_length=100, use_unicode=False):
    global __unicode_alphabet
    if length is None:
        length = random.randint(1, max_length)
    
    if use_unicode and __unicode_alphabet is None:
        # this idea is from http://stackoverflow.com/questions/1477294/generate-random-utf-8-string-in-python
        __unicode_alphabet = [get_char(code_point) for lower, upper in [
            ( 0x0021, 0x0021 ), ( 0x0023, 0x0026 ), ( 0x0028, 0x007E ), ( 0x00A1, 0x00AC ),
            ( 0x00AE, 0x00FF ), ( 0x0100, 0x017F ), ( 0x0180, 0x024F ), ( 0x2C60, 0x2C7F ),
            ( 0x16A0, 0x16F0 ), ( 0x0370, 0x0377 ), ( 0x037A, 0x037E ), ( 0x0384, 0x038A ),
            ( 0x038C, 0x038C )
        ] for code_point in range(lower, upper + 1)]
    
    alphabet = __unicode_alphabet if use_unicode else string.ascii_uppercase + string.digits
    
    return ''.join(random.choice(alphabet) for _ in range(length))

class BackupTesting(rdb_unittest.RdbTestCase):
    servers           = 1
    tables            = 3
    recordsToGenerate = 200
    batchsize         = 40
    
    def add_data(self, db, table, primary_key='id', records=None, batchsize=None):
        if records is None:
            records = self.records
        if batchsize is None:
            batchsize = self.batchsize
        
        # - create db/table
        self.r.expr([db]).set_difference(self.r.db_list()).for_each(self.r.db_create(self.r.row)).run(self.conn)
        self.r.expr([table]).set_difference(self.r.db(db).table_list()).for_each(self.r.db(db).table_create(self.r.row)).run(self.conn)
        
        # - ensure primary_key
        self.assertEqual(self.r.db(db).table(table).info()['primary_key'], primary_key, 'Table %s.%s did not have the expected primary key: %s' % (db, table, primary_key))
        
        # - clear out everything
        self.r.db(db).table(table).delete().run(self.conn)
        
        # - insert random data
        lastStep = 0
        for _ in range(0, records, batchsize):
            batch = []
            for _ in range(lastStep, min(records, lastStep + batchsize)):
                batch.append({
                    primary_key:self.r.uuid(),
                    'bool':     random.choice((False, True)),
                    'integer':  random.randint(-500, 500),
                    'float':    random.uniform(-100000, 100000),
                    'string':   randomString(use_unicode=False),
                    'unicode':  randomString(use_unicode=True),
                    'dict': {
                        'a':    randomString(length=10, use_unicode=False),
                        'b':    randomString(length=10, use_unicode=True)
                    },
                    'array': [
                        randomString(length=10, use_unicode=False),
                        randomString(length=10, use_unicode=True)
                    ],
                    'complicated': {
                        'a': [randomString(length=10, use_unicode=False), random.randint(-500, 500), random.choice((False, True))],
                        '2': [
                            {
                                'a': randomString(length=10, use_unicode=False),
                                'b': random.randint(-500, 500),
                                'c': random.choice((False, True))
                            }, {
                                'a': randomString(length=10, use_unicode=False),
                                'b': random.randint(-500, 500),
                                'c': random.choice((False, True))
                            }
                        ]
                    },
                    'binary':    self.r.binary(bytes(bytearray(random.getrandbits(8) for _ in xrange(10)))),
                    'time':      self.r.time(
                                    random.randint(1400, 9999),
                                    random.randint(1, 12),
                                    random.randint(1, 28),
                                    random.randint(0, 23),
                                    random.randint(0, 59),
                                    random.uniform(0, 59),
                                    random.choice(('Z', '+01:00', '-05:00', '+10:00', '-11:00'))
                                ),
                    'circle':   self.r.circle([random.uniform(-90, 90), random.uniform(-180, 180)], random.uniform(1, 1000))
                })
            self.r.db(db).table(table).insert(batch).run(self.conn)
        
        # - create indexes
        self.r.db(db).table(table).index_create('bool').run(self.conn)
        self.r.db(db).table(table).index_create('unicode').run(self.conn)
        self.r.db(db).table(table).index_create('complicated').run(self.conn)
        self.r.db(db).table(table).index_create('function', self.r.row['dict']['a']).run(self.conn)
        self.r.db(db).table(table).index_wait().run(self.conn)
    
    def compare_servers(self, targetConn):
        self.maxDiff = None
        for db, table, primary_key in self.r.db('rethinkdb').table('table_config').pluck('db', 'name', 'primary_key').map(self.r.row.values()).run(self.conn):
            # - confirm table existance and metadata
            targetInfo = None
            try:
                targetInfo = self.r.db(db).table(table).info().run(targetConn)
            except self.r.ReqlOpFailedError as e:
                raise Exception('Table %s.%s does not exist on the target server: %s' % (db, table, str(e)))
            
            # - compare the indexes
            actualIndexes = dict((x['index'], x) for x in self.r.db(db).table(table).index_status().run(targetConn))
            for expected in self.r.db(db).table(table).index_status().run(self.conn):
                self.assertTrue(expected['index'] in actualIndexes, 'Did not find the index < %s > in %s' % (expected['index'], actualIndexes))
                actual = actualIndexes[expected['index']]
                for value in ['function', 'geo', 'multi', 'outdated', 'query', 'ready']:
                    if not actual[value] == expected[value]:
                        self.error('On table %s.%s index %s the %s did not match, got < %s > when expected %s' % (db, table, expected['index'], value, str(actual[value]), str(expected[value])))
            
            # - compare the keys
            source = list(self.r.db(db).table(table).order_by(index=primary_key)[primary_key].run(self.conn))
            target = list(self.r.db(db).table(table).order_by(index=primary_key)[primary_key].run(targetConn))
            sys.stdout.flush()
            self.assertEqual(source, target)
            
            # - compare the data
            source = self.r.db(db).table(table).order_by(index=primary_key).run(self.conn)
            target = self.r.db(db).table(table).order_by(index=primary_key).run(targetConn)
            
            for expected, actual, i in zip(source, target, counter()):
                self.assertEqual(actual, expected, 'Expected did not match actual in table %s.%s item %d. Expected:\n%s\nvs. Actual:\n%s' % (db, table, i, pprint.pformat(expected), pprint.pformat(actual)))
            
            # ensure the cursors have been drained on both
            try:
                source.next()
            except self.r.ReqlCursorEmpty: pass
            else:
                raise Exception('In %s.%s there were more entries in the source than target (%d)' % (db, table, i))
            try:
                target.next()
            except self.r.ReqlCursorEmpty: pass
            else:
                raise Exception('In %s.%s there were more entries in the target than source (%d)' % (db, table, i))
    
    
    
    def test_dump_restore(self):
        sourceServer  = self.cluster[0]
        targetCluster = driver.Cluster(initial_servers=1)
        targetServer  = targetCluster[0]
        targetConn    = self.r.connect(host=targetServer.host, port=str(targetServer.driver_port))
        
        # - add tables/records
        for i in range(self.tables):
            self.add_data(db=self.dbName, table='roundtrip_%d' % i)
        
        # - dump to file
        dumpFile = tempfile.mktemp(suffix='.tar.gz')
        self.assertEqual(0, self.r._dump.main(argv=[
            '--quiet', '--debug', '--host-name', sourceServer.host, '--driver-port', str(sourceServer.driver_port),
            '-f', dumpFile
        ]))
        utils.cleanupPathAtExit(dumpFile)
        
        # - restore to a second server
        self.assertEqual(0, self.r._restore.main(argv=[
            '--quiet', '--debug', '--host-name', targetServer.host, '--driver-port', str(targetServer.driver_port),
            dumpFile
        ]))
        
        # - validate that all tables match
        self.compare_servers(targetConn)
        
        # - restore again with --force
        self.assertEqual(0, self.r._restore.main(argv=[
            '--quiet', '--debug', '--host-name', targetServer.host, '--driver-port', str(targetServer.driver_port),
            dumpFile, '--force'
        ]))
    
    def test_export_import_csv(self):
        sourceServer  = self.cluster[0]
        targetCluster = driver.Cluster(initial_servers=1)
        targetServer  = targetCluster[0]
        targetConn    = self.r.connect(host=targetServer.host, port=str(targetServer.driver_port))
        
        outputFolder  = tempfile.mkdtemp()
        utils.cleanupPathAtExit(outputFolder)
        outputFolder  = os.path.join(outputFolder, 'export')
        
        # - add tables/records
        self.add_data(db=self.dbName, table=self.tableName)
        
        # - export the table
        self.assertEqual(0, self.r._export.main(argv=[
            '--quiet', '--debug', '--host-name', sourceServer.host, '--driver-port', str(sourceServer.driver_port),
            '-d', outputFolder, '-e', '%s.%s' % (self.dbName, self.tableName), '--format', 'csv',
            '--fields', 'bool,integer,float,string,unicode,dict,array,complicated,binary,time,circle'
        ]))
        
        # - verify existance of files
        outputBasePath = os.path.join(outputFolder, self.dbName, self.tableName)
        self.assertTrue(os.path.exists(outputBasePath + '.info'), 'There was no file at: %s' % (outputBasePath + '.info'))
        self.assertTrue(os.path.exists(outputBasePath + '.csv'), 'There was no file at: %s' % (outputBasePath + '.csv'))
        
        # - import the table
        self.assertEqual(0, self.r._import.main(argv=[
            '--quiet', '--debug', '--host-name', targetServer.host, '--driver-port', str(targetServer.driver_port),
            '-f', outputBasePath + '.csv', '--table', '%s.%s' % (self.dbName, self.tableName)
        ]))
        
        # - validate that the table matches
        self.compare_servers(targetConn)
    
    def test_export_import_json(self):
        sourceServer  = self.cluster[0]
        targetCluster = driver.Cluster(initial_servers=1)
        targetServer  = targetCluster[0]
        targetConn    = self.r.connect(host=targetServer.host, port=str(targetServer.driver_port))
        
        outputFolder  = tempfile.mkdtemp()
        utils.cleanupPathAtExit(outputFolder)
        outputFolder  = os.path.join(outputFolder, 'export')
        
        # - add tables/records
        self.add_data(db=self.dbName, table=self.tableName)
        
        # - export the table
        self.assertEqual(0, self.r._export.main(argv=[
            '--quiet', '--debug', '--host-name', sourceServer.host, '--driver-port', str(sourceServer.driver_port),
            '-d', outputFolder, '-e', '%s.%s' % (self.dbName, self.tableName), '--format', 'json'
        ]))
        
        # - verify existance of files
        outputBasePath = os.path.join(outputFolder, self.dbName, self.tableName)
        self.assertTrue(os.path.exists(outputBasePath + '.info'), 'There was no file at: %s' % (outputBasePath + '.info'))
        self.assertTrue(os.path.exists(outputBasePath + '.json'), 'There was no file at: %s' % (outputBasePath + '.json'))
        
        # - import the table
        self.assertEqual(0, self.r._import.main(argv=[
            '--quiet', '--debug', '--host-name', targetServer.host, '--driver-port', str(targetServer.driver_port),
            '-f', outputBasePath + '.json', '--table', '%s.%s' % (self.dbName, self.tableName)
        ]))
        
        # - validate that the table matches
        self.compare_servers(targetConn)
    
    def test_index_rebuild(self):
        sourceServer  = self.cluster[0]
        
        # - add tables/records
        self.add_data(db=self.dbName, table=self.tableName)
        
        # - rebuild indexes
        self.assertEqual(0, self.r._index_rebuild.main(argv=[
            '--quiet', '--debug', '--host-name', sourceServer.host, '--driver-port', str(sourceServer.driver_port)
        ]))
        
        # - rebuild indexes with force
        self.assertEqual(0, self.r._index_rebuild.main(argv=[
            '--quiet', '--debug', '--host-name', sourceServer.host, '--driver-port', str(sourceServer.driver_port),
            '--force'
        ]))
        
        # - rebuild indexes on the specific table with force
        self.assertEqual(0, self.r._index_rebuild.main(argv=[
            '--quiet', '--debug', '--host-name', sourceServer.host, '--driver-port', str(sourceServer.driver_port),
            '--force', '--rebuild', '%s.%s' % (self.dbName, self.tableName)
        ]))
    
if __name__ == '__main__':
    rdb_unittest.main()
